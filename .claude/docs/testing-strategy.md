# Testing Strategy for Python Deadlines

<document_type>Technical Documentation</document_type>
<priority>HIGH</priority>
<last_updated>2025-09-15</last_updated>

## ğŸ“‹ Overview

This document outlines the comprehensive testing strategy for the Python Deadlines project, covering unit tests, integration tests, end-to-end tests, and quality assurance practices.

## ğŸ¯ Testing Goals

### Coverage Targets
- **Python Backend**: 85% code coverage
- **JavaScript Frontend**: 80% code coverage
- **E2E Test Scenarios**: 20+ critical user journeys
- **Data Integrity**: 100% schema validation

### Quality Metrics
- Zero regression bugs in production
- Sub-5 second test execution for unit tests
- Sub-30 second execution for integration tests
- Automated testing on every PR

## ğŸ—ï¸ Test Architecture

### Test Pyramid
```
         /\
        /E2E\        (10%) - Critical user journeys
       /------\
      /Integr. \     (30%) - Component interactions
     /----------\
    / Unit Tests \   (60%) - Individual functions
   /--------------\
```

### Test Types

#### 1. Unit Tests
**Purpose**: Test individual functions and modules in isolation

**Python Unit Tests** (`tests/*.py`):
- Data processing functions
- Schema validation
- Import/export utilities
- Newsletter generation
- Git integration

**JavaScript Unit Tests** (`tests/frontend/unit/*.test.js`):
- UI component logic
- State management
- Event handlers
- Utility functions

#### 2. Integration Tests
**Purpose**: Test component interactions and data flow

**Backend Integration** (`tests/integration/`):
- Jekyll build validation
- Data pipeline end-to-end
- External API interactions
- Multi-language generation

**Frontend Integration**:
- Component mounting
- State synchronization
- API communication
- Browser compatibility

#### 3. End-to-End Tests
**Purpose**: Test complete user workflows

**E2E Scenarios** (`tests/e2e/specs/`):
- Search functionality
- Conference filtering
- Favorites management
- Calendar export
- Notification system
- Mobile responsiveness

## ğŸ› ï¸ Testing Tools

### Backend Testing Stack
- **pytest**: Python test framework
- **pytest-cov**: Code coverage reporting
- **pytest-mock**: Mocking utilities
- **vladiate**: CSV validation
- **pydantic**: Schema validation

### Frontend Testing Stack
- **Jest**: JavaScript test framework
- **@testing-library**: DOM testing utilities
- **jest-localstorage-mock**: LocalStorage mocking
- **Playwright**: E2E browser automation

### CI/CD Integration
- **GitHub Actions**: Automated test execution
- **Codecov**: Coverage tracking
- **Pre-commit hooks**: Local validation

## ğŸ“ Test File Organization

```
tests/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ unit/              # JavaScript unit tests
â”‚   â”‚   â”œâ”€â”€ search.test.js
â”‚   â”‚   â”œâ”€â”€ favorites.test.js
â”‚   â”‚   â””â”€â”€ notifications.test.js
â”‚   â”œâ”€â”€ integration/       # Frontend integration tests
â”‚   â””â”€â”€ utils/            # Test utilities and helpers
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ specs/            # E2E test specifications
â”‚   â”‚   â”œâ”€â”€ search-functionality.spec.js
â”‚   â”‚   â”œâ”€â”€ conference-filters.spec.js
â”‚   â”‚   â””â”€â”€ countdown-timers.spec.js
â”‚   â””â”€â”€ utils/            # E2E helpers
â”œâ”€â”€ integration/          # Backend integration tests
â”‚   â”œâ”€â”€ test_jekyll_build.py
â”‚   â””â”€â”€ test_data_pipeline.py
â”œâ”€â”€ regression/           # Regression test suite
â””â”€â”€ performance/         # Performance benchmarks
```

## ğŸš€ Running Tests

### Quick Test Commands
```bash
# Python tests
pixi run test           # Run all Python tests
pixi run test-fast     # Stop on first failure
pixi run test-cov      # With coverage report

# JavaScript tests
npm test               # Run all Jest tests
npm run test:watch    # Watch mode
npm run test:coverage # Coverage report

# E2E tests
npm run e2e           # Run all E2E tests
npm run e2e:headed    # Run with browser visible
npm run e2e:debug     # Debug mode

# Integration tests
pixi run test tests/integration/

# Pre-commit validation
pixi run pre          # Run all quality checks
```

### Test Configuration Files
- `jest.config.js`: Jest configuration and coverage thresholds
- `playwright.config.js`: E2E test configuration
- `pytest.ini`: Python test configuration
- `.github/workflows/`: CI test workflows

## ğŸ“Š Coverage Requirements

### JavaScript Coverage Thresholds
```javascript
{
  'notifications.js': { branches: 71, functions: 85, lines: 85 },
  'countdown-simple.js': { branches: 75, functions: 80, lines: 80 },
  'search.js': { branches: 70, functions: 75, lines: 75 },
  'favorites.js': { branches: 70, functions: 80, lines: 80 },
  'dashboard.js': { branches: 60, functions: 70, lines: 70 },
  'conference-manager.js': { branches: 65, functions: 75, lines: 75 },
  'conference-filter.js': { branches: 65, functions: 70, lines: 70 }
}
```

### Python Coverage Requirements
- Core utilities: 85% minimum
- Data processing: 90% minimum
- Import functions: 80% minimum
- Schema validation: 100% required

## ğŸ”„ Continuous Integration

### GitHub Actions Workflows

#### Frontend Tests (`frontend-tests.yml`)
- Runs on: Push to main, PRs affecting JS/tests
- Matrix: Node 18, 20
- Coverage: Uploaded to Codecov
- Comments: PR with coverage report

#### E2E Tests (`e2e-tests.yml`)
- Runs on: Push to main, PRs
- Browsers: Chromium, Firefox, WebKit
- Mobile: iOS Safari, Android Chrome
- Visual regression: Screenshot comparisons

#### Python Tests (via pre-commit)
- Runs on: Every commit
- Validates: Schema, data integrity, processing logic
- Coverage: Tracked and enforced

## ğŸ› Test Data Management

### Fixtures
**Python Fixtures** (`conftest.py`):
- `sample_conference`: Valid conference data
- `invalid_conference`: Schema violation examples
- `mock_api_response`: External API mocks

**JavaScript Fixtures**:
- Conference DOM elements
- Mock localStorage data
- Timer states

### Test Data Files
- `_data/test_conferences.yml`: Minimal test dataset
- `_config.test.yml`: Fast Jekyll build config
- Mock API responses in `tests/fixtures/`

## ğŸ¯ Critical Test Scenarios

### Must-Pass Tests
1. **Data Integrity**
   - Conference schema validation
   - Date format consistency
   - Timezone handling
   - Archive management

2. **User Workflows**
   - Search and filter conferences
   - Save/remove favorites
   - Export to calendar
   - View countdown timers

3. **Build Process**
   - Jekyll site generation
   - Multi-language builds
   - Search index creation
   - ICS calendar generation

4. **Performance**
   - Page load < 3 seconds
   - Search response < 500ms
   - Data processing < 10 seconds

## ğŸ“ˆ Test Metrics Tracking

### Key Metrics
- Test execution time
- Coverage percentage
- Flaky test rate
- Regression detection rate

### Reporting
- Daily: CI dashboard
- Weekly: Coverage trends
- Monthly: Test health report
- Per PR: Test results comment

## ğŸ” Debugging Failed Tests

### Common Issues and Solutions

**Timezone-related failures**:
```bash
# Set consistent timezone for tests
export TZ='UTC'
npm test
```

**Jekyll build failures**:
```bash
# Use minimal config for faster debugging
bundle exec jekyll build --config _config.yml,_config.test.yml --trace
```

**E2E timeouts**:
```bash
# Run with extended timeouts
npx playwright test --timeout=60000
```

**Coverage threshold failures**:
```bash
# Generate detailed coverage report
npm run test:coverage
open coverage/lcov-report/index.html
```

## ğŸš¨ Test Emergency Procedures

### When Tests Block Deployment
1. Run `pixi run test-fast` to identify failing test
2. Check if it's environment-specific
3. If critical: Fix immediately
4. If non-critical: Skip with `@pytest.mark.skip` and create issue

### Flaky Test Protocol
1. Mark with `@pytest.mark.flaky` or `test.skip()`
2. Add retry logic: `retries: process.env.CI ? 2 : 0`
3. Investigate root cause
4. Fix or remove within 1 week

## ğŸ“ Test Writing Guidelines

### Good Test Practices
âœ… **DO**:
- Write descriptive test names
- Use arrange-act-assert pattern
- Mock external dependencies
- Test edge cases and errors
- Keep tests independent

âŒ **DON'T**:
- Share state between tests
- Test implementation details
- Use production data
- Make real API calls
- Ignore flaky tests

### Test Naming Convention
```javascript
// JavaScript
describe('ComponentName', () => {
  test('should perform expected behavior when condition', () => {})
  test('should handle error when invalid input', () => {})
})
```

```python
# Python
class TestClassName:
    def test_function_success_case(self):
        """Test that function works with valid input."""

    def test_function_error_handling(self):
        """Test that function handles errors gracefully."""
```

## ğŸ”„ Test Maintenance

### Regular Tasks
- **Weekly**: Review flaky tests
- **Monthly**: Update test data
- **Quarterly**: Coverage audit
- **Yearly**: Test strategy review

### Test Refactoring Triggers
- Coverage drops below threshold
- Test execution > 5 minutes
- Duplicate test logic found
- New feature without tests

## ğŸ“š Additional Resources

### Documentation
- [Jest Documentation](https://jestjs.io/docs)
- [Playwright Documentation](https://playwright.dev/docs)
- [Pytest Documentation](https://docs.pytest.org)

### Internal Guides
- `.claude/docs/critical-operations.md`: Data integrity rules
- `.claude/docs/git-workflow.md`: Testing in Git workflow
- `CONTRIBUTING.md`: Test requirements for contributors

## ğŸ“ Testing Best Practices

### The Testing Manifesto
1. **Fast**: Tests should run quickly
2. **Independent**: No shared state
3. **Repeatable**: Same result every time
4. **Self-Validating**: Clear pass/fail
5. **Timely**: Written with or before code

### ADHD-Friendly Testing
- Break large test suites into focused groups
- Use `--fail-fast` for immediate feedback
- Visual test reporters for better focus
- Automated test generation where possible
- Clear error messages with fix suggestions

---

<metadata>
version: 1.0.0
author: Python Deadlines Team
review_cycle: quarterly
compliance: WCAG 2.1, GDPR
</metadata>