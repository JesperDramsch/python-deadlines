name: Sync New URLs to Changedetection.io

on:
  push:
    branches: [main, master]
    paths:
      - '_data/**'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (log actions without making API calls)'
        type: boolean
        default: false
      force_recheck:
        description: 'Check all URLs, not just new ones'
        type: boolean
        default: false

permissions:
  contents: read

concurrency:
  group: sync-changedetection
  cancel-in-progress: false

jobs:
  sync-urls:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install pyyaml

      - name: Find last successful run commit
        id: last-run
        run: |
          last_sha=$(gh run list --workflow="Sync New URLs to Changedetection.io" --json conclusion,headSha --jq '.[] | select(.conclusion=="success") | .headSha' | head -n1)
          if [ -n "$last_sha" ] && [ "${{ inputs.force_recheck }}" != "true" ]; then
            echo "sha=$last_sha" >> $GITHUB_OUTPUT
            echo "Found last successful run at $last_sha"
          else
            echo "sha=$(git rev-list --max-parents=0 HEAD)" >> $GITHUB_OUTPUT
            echo "Using initial commit (full scan mode)"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Check changedetection.io configuration
        id: check_cd
        run: |
          if [ -n "${{ vars.CHANGEDETECTION_URL }}" ] && [ -n "${{ secrets.CHANGEDETECTION_KEY }}" ]; then
            echo "configured=true" >> $GITHUB_OUTPUT
          else
            echo "configured=false" >> $GITHUB_OUTPUT
            echo "::warning::changedetection.io not configured. Set CHANGEDETECTION_URL variable and CHANGEDETECTION_KEY secret."
          fi

      - name: Sync URLs to changedetection.io
        if: steps.check_cd.outputs.configured == 'true'
        env:
          LAST_SHA: ${{ steps.last-run.outputs.sha }}
          DRY_RUN: ${{ inputs.dry_run }}
        run: |
          # Build auth headers (same pattern as archive-new-urls.yml)
          AUTH_HEADERS="-H \"x-api-key: ${{ secrets.CHANGEDETECTION_KEY }}\""
          if [ -n "${{ secrets.CF_ACCESS_CLIENT_ID }}" ]; then
            AUTH_HEADERS="$AUTH_HEADERS -H \"CF-Access-Client-Id: ${{ secrets.CF_ACCESS_CLIENT_ID }}\""
            AUTH_HEADERS="$AUTH_HEADERS -H \"CF-Access-Client-Secret: ${{ secrets.CF_ACCESS_CLIENT_SECRET }}\""
          fi

          API_BASE="${{ vars.CHANGEDETECTION_URL }}/api/v1"

          # Fetch existing watches (unless dry run)
          if [ "$DRY_RUN" != "true" ]; then
            echo "Fetching existing watches..."
            EXISTING_WATCHES=$(eval "curl -sf $AUTH_HEADERS '$API_BASE/watch'") || {
              echo "::error::Failed to fetch existing watches from changedetection.io"
              exit 1
            }

            # Validate we got JSON, not Cloudflare HTML
            if ! echo "$EXISTING_WATCHES" | jq empty 2>/dev/null; then
              echo "::error::API returned invalid response (check CF Access credentials)"
              exit 1
            fi

            echo "$EXISTING_WATCHES" | jq -r '.[] | .url' | tr '[:upper:]' '[:lower:]' | sed 's:/*$::' > /tmp/existing_urls.txt
            echo "Fetched $(wc -l < /tmp/existing_urls.txt) existing watch URLs"
          else
            touch /tmp/existing_urls.txt
          fi

          # Process conferences with Python
          python3 << 'PYTHON_SCRIPT'
          import os
          import re
          import json
          import subprocess
          import yaml
          from urllib.parse import urlparse
          from typing import Optional

          LAST_SHA = os.environ.get('LAST_SHA', '')
          DRY_RUN = os.environ.get('DRY_RUN', 'false').lower() == 'true'

          # Tag UUIDs
          BASE_TAGS = [
              "1ad0fbdf-482a-4296-b983-45311ccf2352",
              "aa19ce68-9d8f-4e86-8df2-342faccbccc8",
          ]

          CONTINENT_TAGS = {
              "africa": "70a6a80d-c13d-450f-b36e-cfa6cbd3fbd4",
              "europe": "23a96495-26fc-4fa6-bdda-78c3051748e5",
              "america": "b44ef3c3-7280-49bc-b71f-4d7c0ca62fb7",
              "asia": "b488faec-899a-4578-9560-7154999b8c9e",
              "oceania": "11742b59-eb4e-42b3-8069-64dfc1bced4e",
          }

          SUB_TO_TAG = {
              "WEB": "7054a0fb-69db-47fd-9c2e-3ee6c638bac7",
              "SCIPY": "571f57a6-258d-4101-9136-cd1643365e55",
              "DATA": "8e55732c-4d04-4457-bff8-2c0c851590db",
          }

          CFP_TAG = "5799f798-2b44-452e-aefa-43874f6c3e1e"

          URL_FIELDS = {
              'link': 'Main',
              'cfp_link': 'CFP',
          }

          YEAR_PATTERN = re.compile(r'/(20(?:2[4-9]|3[0-5]))/')

          def get_continent(lat: float, lon: float) -> Optional[str]:
              if lat is None or lon is None:
                  return None
              if lon < -30:
                  return "america"
              if lat < 0 and lon > 100:
                  return "oceania"
              if lat < 35 and lon < 55:
                  return "africa"
              if lon > 40:
                  return "asia"
              return "europe"

          def get_location_coords(conf: dict) -> tuple[Optional[float], Optional[float]]:
              location = conf.get('location', [])
              if location and isinstance(location, list) and len(location) > 0:
                  loc = location[0]
                  return loc.get('latitude'), loc.get('longitude')
              return None, None

          def get_start_month(conf: dict) -> Optional[int]:
              start = conf.get('start')
              if not start:
                  return None
              if hasattr(start, 'month'):
                  return start.month
              try:
                  parts = str(start).split('-')
                  if len(parts) >= 2:
                      return int(parts[1])
              except (ValueError, IndexError):
                  pass
              return None

          def template_url(url: str, start_month: Optional[int]) -> str:
              if not start_month:
                  return url
              match = YEAR_PATTERN.search(url)
              if not match:
                  return url
              template = f"{{% now 'utc' + 'months={start_month}', '%Y' %}}"
              return YEAR_PATTERN.sub(f'/{template}/', url)

          def build_tags(conf: dict, field: str = 'link') -> list[str]:
              tags = list(BASE_TAGS)
              lat, lon = get_location_coords(conf)
              continent = get_continent(lat, lon)
              if continent and continent in CONTINENT_TAGS:
                  tags.append(CONTINENT_TAGS[continent])
              sub = conf.get('sub', '').upper()
              if sub in SUB_TO_TAG:
                  tags.append(SUB_TO_TAG[sub])
              if field == 'cfp_link':
                  tags.append(CFP_TAG)
              return tags

          def normalize_url(url: str) -> str:
              if not url:
                  return ''
              return url.rstrip('/').lower()

          def get_base_url(url: str) -> str:
              if not url:
                  return ''
              parsed = urlparse(url)
              return f"{parsed.scheme}://{parsed.netloc}"

          def get_url_prefix_before_year(url: str) -> Optional[str]:
              match = YEAR_PATTERN.search(url)
              if not match:
                  return None
              return url[:match.start() + 1]

          # Load existing URLs from file (created by shell above)
          existing_urls = set()
          with open('/tmp/existing_urls.txt', 'r') as f:
              for line in f:
                  url = line.strip()
                  if url:
                      existing_urls.add(url)
                      base = get_base_url(url)
                      if base:
                          existing_urls.add(base.lower())
          print(f"Loaded {len(existing_urls)} existing URLs")

          def url_exists(url: str, use_base_url: bool = True) -> tuple[bool, str]:
              if url.lower() in existing_urls:
                  return True, "exact URL match"
              if use_base_url:
                  base = get_base_url(url)
                  if base and base.lower() in existing_urls:
                      return True, "base URL match"
              url_prefix = get_url_prefix_before_year(url)
              if url_prefix:
                  prefix_lower = url_prefix.lower()
                  for cached_url in existing_urls:
                      if cached_url.startswith(prefix_lower):
                          return True, "templated version exists"
              return False, ""

          def get_old_conferences():
              try:
                  result = subprocess.run(
                      ['git', 'show', f'{LAST_SHA}:_data/conferences.yml'],
                      capture_output=True, text=True
                  )
                  if result.returncode == 0:
                      return yaml.safe_load(result.stdout) or []
              except Exception as e:
                  print(f"Warning: Could not load old conferences: {e}")
              return []

          def get_current_conferences():
              try:
                  with open('_data/conferences.yml', 'r') as f:
                      return yaml.safe_load(f) or []
              except Exception as e:
                  print(f"Error loading conferences: {e}")
              return []

          print("=" * 60)
          print("Processing Conference URLs")
          print("=" * 60)

          if DRY_RUN:
              print("*** DRY RUN MODE ***")

          print(f"Comparing against commit: {LAST_SHA[:8]}...")

          old_conferences = get_old_conferences()
          new_conferences = get_current_conferences()

          print(f"Old conferences: {len(old_conferences)}")
          print(f"Current conferences: {len(new_conferences)}")

          old_urls = set()
          for conf in old_conferences:
              for field in URL_FIELDS.keys():
                  url = conf.get(field)
                  if url:
                      old_urls.add(normalize_url(url))

          print(f"URLs in previous version: {len(old_urls)}")

          watches_to_add = []
          skipped = []

          for conf in new_conferences:
              conf_name = f"{conf.get('conference', 'Unknown')} {conf.get('year', '')}".strip()
              start_month = get_start_month(conf)

              for field, label in URL_FIELDS.items():
                  url = conf.get(field)
                  if not url:
                      continue

                  title = f"{conf_name} - {label}"

                  if normalize_url(url) in old_urls:
                      skipped.append(f"{title}: existed in previous version")
                      continue

                  use_base_url = (field == 'link')
                  exists, reason = url_exists(url, use_base_url=use_base_url)
                  if exists:
                      skipped.append(f"{title}: {reason}")
                      continue

                  final_url = template_url(url, start_month)
                  tags = build_tags(conf, field)

                  watches_to_add.append({
                      'url': final_url,
                      'original_url': url if final_url != url else None,
                      'title': title,
                      'tags': tags,
                  })

          print(f"\nTo add: {len(watches_to_add)}")
          print(f"Skipped: {len(skipped)}")

          with open('/tmp/watches_to_add.json', 'w') as f:
              json.dump(watches_to_add, f)

          with open('/tmp/skipped.json', 'w') as f:
              json.dump(skipped, f)

          for watch in watches_to_add:
              print(f"  ADD: {watch['title']}")
              if watch['original_url']:
                  print(f"       Templated: {watch['url']}")
          PYTHON_SCRIPT

          # Now add watches using curl (same shell, /tmp persists)
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Changedetection.io Sync Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "$DRY_RUN" == "true" ]; then
            echo "### Dry Run Mode" >> $GITHUB_STEP_SUMMARY
            echo "The following watches would be added:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            jq -r '.[] | "- **\(.title)** - \(.url)"' /tmp/watches_to_add.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Total:** $(jq length /tmp/watches_to_add.json) watches (dry run)" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          WATCH_COUNT=$(jq length /tmp/watches_to_add.json)
          if [ "$WATCH_COUNT" -eq 0 ]; then
            echo "No new watches to add"
            echo "No new watches to add." >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          ADDED=0
          FAILED=0

          while read -r watch; do
            URL=$(echo "$watch" | jq -r '.url')
            TITLE=$(echo "$watch" | jq -r '.title')
            TAGS=$(echo "$watch" | jq -c '.tags')

            # Build payload with jq (handles escaping properly)
            PAYLOAD=$(jq -n --arg url "$URL" --arg title "$TITLE" --argjson tags "$TAGS" \
              '{url: $url, title: $title, tags: $tags}')

            echo "Adding: $TITLE"
            echo "  URL: $URL"

            # Write payload to temp file to avoid quoting issues with eval
            echo "$PAYLOAD" > /tmp/payload.json

            # Use eval with file input, exactly like archive-new-urls.yml pattern
            RESPONSE=$(eval "curl -sf -X POST $AUTH_HEADERS -H 'Content-Type: application/json' -d @/tmp/payload.json '$API_BASE/watch'") && {
              if echo "$RESPONSE" | jq -e '.uuid' >/dev/null 2>&1; then
                echo "  ✅ Added successfully"
                echo "- ✅ **$TITLE** - $URL" >> $GITHUB_STEP_SUMMARY
                ADDED=$((ADDED + 1))
              else
                echo "  ❌ Unexpected response: $RESPONSE"
                echo "- ❌ **$TITLE** - $URL (unexpected response)" >> $GITHUB_STEP_SUMMARY
                FAILED=$((FAILED + 1))
              fi
            } || {
              echo "  ❌ Failed to add"
              echo "- ❌ **$TITLE** - $URL (failed)" >> $GITHUB_STEP_SUMMARY
              FAILED=$((FAILED + 1))
            }

            sleep 1
          done < <(jq -c '.[]' /tmp/watches_to_add.json)

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f /tmp/skipped.json ] && [ "$(jq length /tmp/skipped.json)" -gt 0 ]; then
            echo "<details>" >> $GITHUB_STEP_SUMMARY
            echo "<summary>Skipped ($(jq length /tmp/skipped.json) URLs)</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            jq -r '.[]' /tmp/skipped.json | while read -r item; do
              echo "- $item" >> $GITHUB_STEP_SUMMARY
            done
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "**Commit range:** ${{ steps.last-run.outputs.sha }}..HEAD" >> $GITHUB_STEP_SUMMARY
