name: Archive New Conference URLs

permissions:
  contents: read

on:
  push:
    branches: [ main, master]
    paths:
      - '_data/**'
  schedule:
    # Runs at 00:00 UTC on Saturday
    - cron: '0 0 * * 6'
  workflow_dispatch:
    inputs:
      skip_archive:
        description: 'Skip Web Archive (only add to changedetection)'
        type: boolean
        default: false
      skip_changedetection:
        description: 'Skip changedetection.io (only archive)'
        type: boolean
        default: false

jobs:
  check-new-urls:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0 # Full history for finding last run

      - name: Find last successful run commit
        id: last-run
        run: |
          last_sha=$(gh run list --workflow="Archive New Conference URLs" --json conclusion,headSha --jq '.[] | select(.conclusion=="success") | .headSha' | head -n1)
          if [ -n "$last_sha" ]; then
            echo "sha=$last_sha" >> $GITHUB_OUTPUT
          else
            echo "sha=$(git rev-list --max-parents=0 HEAD)" >> $GITHUB_OUTPUT
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract new conference entries
        id: extract
        run: |
          # Get the diff for conferences.yml
          DIFF=$(git diff ${{ steps.last-run.outputs.sha }} -- _data/conferences.yml || true)

          if [ -z "$DIFF" ]; then
            echo "No changes to conferences.yml"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "has_changes=true" >> $GITHUB_OUTPUT

          # Parse new conference entries from the diff
          # We need: link, conference name, place, sub
          python3 << 'EOF'
          import yaml
          import subprocess
          import json
          import re

          # Get current and previous versions
          result = subprocess.run(
              ['git', 'show', '${{ steps.last-run.outputs.sha }}:_data/conferences.yml'],
              capture_output=True, text=True
          )
          old_data = yaml.safe_load(result.stdout) if result.returncode == 0 else []

          with open('_data/conferences.yml', 'r') as f:
              new_data = yaml.safe_load(f)

          # Create sets of existing URLs (normalize trailing slashes)
          def normalize_url(url):
              return url.rstrip('/').lower() if url else ''

          old_urls = {normalize_url(c.get('link', '')) for c in (old_data or [])}

          # Find new entries
          new_entries = []
          for conf in (new_data or []):
              url = normalize_url(conf.get('link', ''))
              if url and url not in old_urls:
                  new_entries.append({
                      'url': conf.get('link', ''),
                      'name': f"{conf.get('conference', 'Unknown')} {conf.get('year', '')}".strip(),
                      'place': conf.get('place', ''),
                      'sub': conf.get('sub', 'PY'),
                  })

          # Output as JSON for later steps
          print(f"Found {len(new_entries)} new conference URLs")
          with open('/tmp/new_conferences.json', 'w') as f:
              json.dump(new_entries, f)

          # Also output just URLs for backward compatibility
          urls = [e['url'] for e in new_entries]
          with open('/tmp/new_urls.txt', 'w') as f:
              f.write('\n'.join(urls))
          EOF

          # Set outputs
          if [ -s /tmp/new_urls.txt ]; then
            echo "url_count=$(wc -l < /tmp/new_urls.txt)" >> $GITHUB_OUTPUT
          else
            echo "url_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Archive to Wayback Machine
        if: steps.extract.outputs.has_changes == 'true' && steps.extract.outputs.url_count != '0' && inputs.skip_archive != true
        run: |
          echo "## Web Archive Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          while IFS= read -r url; do
            [ -z "$url" ] && continue
            echo "Archiving: $url"

            response=$(curl -s -L -X POST \
              -A "Python-Deadlines-Bot/1.0 (https://pythondeadlin.es; admin@python-deadlin.es)" \
              "https://web.archive.org/save/$url" || true)
            status=$?

            if [ $status -eq 0 ]; then
              echo "✓ Successfully archived: $url"
              echo "- ✅ $url" >> $GITHUB_STEP_SUMMARY
            else
              echo "⚠ Failed to archive: $url (error $status)"
              echo "- ⚠️ $url (failed)" >> $GITHUB_STEP_SUMMARY
            fi

            sleep 5
          done < /tmp/new_urls.txt

      - name: Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Total new URLs:** ${{ steps.extract.outputs.url_count }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit range:** ${{ steps.last-run.outputs.sha }}..HEAD" >> $GITHUB_STEP_SUMMARY
